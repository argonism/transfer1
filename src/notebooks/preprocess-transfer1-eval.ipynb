{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1931b00-c8c4-4d9c-822c-205bb86d9a65",
   "metadata": {},
   "source": [
    "# Preprocessing of NTCIR-17 Transfer Task Eval Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deec37a-3388-4195-8d35-2d4c0a4d21de",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "NTCIR-17 Transfer Task uses the following test collection as the evaluation dataset.\n",
    "\n",
    "### Overview of NTCIR-2 AdHoc Test Collection\n",
    "\n",
    "- Reference\n",
    "> Kando, et al. (2001). [Overview of Japanese and English Information Retrieval Tasks (JEIR) at the Second NTCIR Workshop](http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings2/ovview-kando2.pdf). In: Proceedings of the Second NTCIR Workshop on Research in Chinese & Japanese Text Retrieval and Text Summarization, May 2000- March 2001.\n",
    "- How to obtain the data: [Research Purpose Use of NTCIR Test Collections or Data Archive/ User Agreement](http://research.nii.ac.jp/ntcir/permission/perm-en.html#ntcir-2)\n",
    "> The collection includes (1) Document data (Author abstracts of the Academic Conference Paper Database (1997-1999) and Grant Reports (1988-1997) = about 400,000 Japanese and 130,000 English documents,) (2) 49 Search topics (Japanese and English,) and (3) Relevance Judgements. The whole test collection is available for research purpose use from NII For experiments, the document data must be used with those of the NTCIR-1. Relevance judgments were done of the merged database of NTCIR-1 and NTCIR-2. To merge document collections, the document IDs in the NTCIR-1 must be converted using the script included in the NTCIR-2 CD-ROM. At the Second NTCIR Workshop, segmented data, in which the whole document data were segmented into terms (short units as well as longer units) using the standard software for segmentation in the year of 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b44087-d194-4455-b24f-b82f187e5bd5",
   "metadata": {},
   "source": [
    "## Previous Step\n",
    "\n",
    "- `preprocess-transfer1-train.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26c1c8-1e00-44d7-8e82-de1815b8600c",
   "metadata": {},
   "source": [
    "## Data path\n",
    "- Get a copy of the test collection based on the above instruction.\n",
    "- We assume that the downloaded file has been uncompressed to the following path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3114bc79-eb55-4096-b709-b2a2720fd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DATA1'] = '../testcollections/ntcir/NTCIR-1'\n",
    "os.environ['DATA2'] = '../testcollections/ntcir/NTCIR-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f8308e-4df2-401b-be7a-f802e7dccbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agreem2-e.pdf\t\t j-docs.tgz\t\t      readme-j.pdf\n",
      "agreem2-j.pdf\t\t manual-e.pdf\t\t      readme-j.txt\n",
      "correction-e-130709.pdf  manual-j.pdf\t\t      rels.tgz\n",
      "correction-j-130705.pdf  readme-e-revised-130709.pdf  scripts.tgz\n",
      "e-docs.tgz\t\t readme-e.txt\t\t      topics\n",
      "j-docs\t\t\t readme-j-revised-130709.pdf  topics.tgz\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1ab8f-a0be-49dc-904a-5f1a060b5403",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing of corpus files\n",
    "\n",
    "- NTCIR-2 uses both the new corpus files and that of NTCIR-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87988c62-2372-4baf-98a6-9a7782a05b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j-docs/\n",
      "j-docs/ntc2-j1g\n",
      "j-docs/ntc2-j1k\n"
     ]
    }
   ],
   "source": [
    "!tar xvfz $DATA2/j-docs.tgz -C $DATA2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f285ed86-eb99-4dde-b81e-6814cd4f2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!iconv -f EUC-JP -t UTF-8 -c $DATA2/j-docs/ntc2-j1g > $DATA2/j-docs/ntc2-j1g.utf8\n",
    "!iconv -f EUC-JP -t UTF-8 -c $DATA2/j-docs/ntc2-j1k > $DATA2/j-docs/ntc2-j1k.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae32c7e0-9b81-406d-8383-4b048c477540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403240\n"
     ]
    }
   ],
   "source": [
    "# Number of documents\n",
    "!grep \"^<ACCN\" $DATA2/j-docs/ntc2-j1*.utf8 | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfafde-8fd6-49e4-aa9c-17f2f9ca9e98",
   "metadata": {},
   "source": [
    "### ntc2-j1g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bb45d4-c438-4d70-8c89-412397490984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "def docs_g_jsonl(in_file):\n",
    "    out_file = in_file + '.jsonl'\n",
    "    with open(in_file, 'r') as f, open(out_file, 'w') as f1:\n",
    "        record = ''\n",
    "        items = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line == '</REC>':\n",
    "                accn = re.findall(r'<ACCN>(.+?)<', record)[0]\n",
    "                titl = re.findall(r'<TITL .+?>(.+?)<', record)[0]\n",
    "                abst = re.findall(r'<ABST .+?>(.+?)</ABST>', record)[0]\n",
    "                abst = re.sub(r'<ABST.P>', '', abst)\n",
    "                abst = re.sub(r'</ABST.P>', '', abst)\n",
    "                contents = titl + ' ' + abst\n",
    "                items = {\n",
    "                    'doc_id': accn,\n",
    "                    'text': contents\n",
    "                }\n",
    "                j = json.dumps(items, ensure_ascii=False)\n",
    "                f1.write(f'{j}\\n')\n",
    "                record = ''\n",
    "                items = {}\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'{count}, ', end='', file=sys.stderr)\n",
    "            else:\n",
    "                record += line\n",
    "        print(f'{count}, Done!', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef08dbc-d396-4b9a-ab0f-257f1df013de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 116177, Done!\n"
     ]
    }
   ],
   "source": [
    "docs_g_jsonl(os.getenv('DATA2') + '/j-docs/ntc2-j1g.utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755e114-cbdf-4043-a30a-6af0f649bdf6",
   "metadata": {},
   "source": [
    "### ntc2-j1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75310ae-0c79-4fcd-8030-75a348b1e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import json\n",
    "def docs_k_jsonl(in_file):\n",
    "    out_file = in_file + '.jsonl'\n",
    "    with open(in_file, 'r') as f, open(out_file, 'w') as f1:\n",
    "        record = ''\n",
    "        items = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line == '</REC>':\n",
    "                accn = re.findall(r'<ACCN>(.+?)<', record)[0]\n",
    "                titl = re.findall(r'<PJNM .+?>(.+?)<', record)[0] # Difference\n",
    "                abst = re.findall(r'<ABST .+?>(.+?)</ABST>', record)[0]\n",
    "                abst = re.sub(r'<ABST.P>', '', abst)\n",
    "                abst = re.sub(r'</ABST.P>', '', abst)\n",
    "                contents = titl + ' ' + abst\n",
    "                items = {\n",
    "                    'doc_id': accn,\n",
    "                    'text': contents\n",
    "                }\n",
    "                j = json.dumps(items, ensure_ascii=False)\n",
    "                f1.write(f'{j}\\n')\n",
    "                record = ''\n",
    "                items = {}\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'{count}, ', end='', file=sys.stderr)\n",
    "            else:\n",
    "                record += line\n",
    "        print(f'{count}, Done!', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcc8207-bfa2-473d-af2c-572605bd5e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 287063, Done!\n"
     ]
    }
   ],
   "source": [
    "docs_k_jsonl(os.getenv('DATA2') + '/j-docs/ntc2-j1k.utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fde5424-425c-4b18-9156-e1de89ab875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   116177 ../testcollections/ntcir/NTCIR-2/j-docs/ntc2-j1g.utf8.jsonl\n",
      "   287063 ../testcollections/ntcir/NTCIR-2/j-docs/ntc2-j1k.utf8.jsonl\n",
      "   403240 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l $DATA2/j-docs/ntc2-j1*.utf8.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286e525-e3b9-4881-9fef-fc59d78d12a5",
   "metadata": {},
   "source": [
    "### NTCIR-1 corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e54251-4938-4b06-b7ba-3198ebf9a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "def convert_ntcir1_to_ntcir2(in_file, out_file):\n",
    "    with open(in_file, 'r') as f, open(out_file, 'w') as f2:\n",
    "        for i, line in enumerate(f):\n",
    "            j = json.loads(line)\n",
    "            docid = j['doc_id'].replace('gakkai-', 'gakkai-j-')\n",
    "            j['doc_id'] = docid\n",
    "            jline = json.dumps(j, ensure_ascii=False)\n",
    "            f2.write(f'{jline}\\n')\n",
    "            if i % 10000 == 0:\n",
    "                print(f'{i}, ', end='', file=sys.stderr)\n",
    "        print(f'{i}, Done!', file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74f935b-2c74-4767-9a35-b6f55fba4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000, 320000, 330000, 332917, Done!\n"
     ]
    }
   ],
   "source": [
    "convert_ntcir1_to_ntcir2(\n",
    "    os.getenv('DATA1') + '/mlir/ntc1-j1.utf8.jsonl',\n",
    "    os.getenv('DATA2') + '/j-docs/ntc1-j1.utf8.mod.jsonl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e44364e2-2830-436d-b046-48e85a8adfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $DATA2/j-docs/ntc1-j1.utf8.mod.jsonl $DATA2/j-docs/ntc2-j1g.utf8.jsonl $DATA2/j-docs/ntc2-j1k.utf8.jsonl > $DATA2/j-docs/ntc12-j1gk.mod.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa084101-3968-4094-9a4f-d0aa2c0fa7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntc1-j1.utf8.mod.jsonl\tntc2-j1g       ntc2-j1g.utf8.jsonl  ntc2-j1k.utf8\n",
      "ntc12-j1gk.mod.jsonl\tntc2-j1g.utf8  ntc2-j1k\t\t    ntc2-j1k.utf8.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA2/j-docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81262343-ca48-4196-997f-24e08f47fd5a",
   "metadata": {},
   "source": [
    "## Preprocessing of topic files\n",
    "\n",
    "**WARNING FOR NTCIR-17 TRANSFER TASK PARTICIPANTS**\n",
    "\n",
    "- You are NOT allowed to access the eval dataset topics until you freeze the development of your systems.\n",
    "- Use the train dataset topics for all your development.\n",
    "- See Getting Started at https://github.com/orgs/ntcirtransfer/discussions/ for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f8b3153-83db-4119-8533-301f9b19cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics/\n",
      "topics/topic-e0101-0149\n",
      "topics/topic-j0101-0149\n"
     ]
    }
   ],
   "source": [
    "!tar xvfz $DATA2/topics.tgz -C $DATA2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47fa4e4b-c144-4b68-8740-0547371e1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!iconv -f EUC-JP -t UTF-8 -c $DATA2/topics/topic-j0101-0149 > $DATA2/topics/topic-j0101-0149.utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4519a5d-6e06-4395-8cf4-0a5127c1c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-e0101-0149  topic-j0101-0149  topic-j0101-0149.utf8\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA2/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b309f152-4b8b-42fd-95a0-a82cde1809ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def topics_jsonl(in_file):\n",
    "    out_file = in_file + '.jsonl'\n",
    "    with open(in_file, 'r') as f:\n",
    "        s = f.read()\n",
    "        qid = re.findall('<TOPIC q=([^>]+)>', s)\n",
    "        title = re.findall('<TITLE>\\n(.*)\\n<\\/TITLE>', s)\n",
    "        desc = re.findall('<DESCRIPTION>\\n(.*)\\n<\\/DESCRIPTION>', s)\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(qid)):\n",
    "            f.write(f'{{ \"query_id\": \"{qid[i]}\", \"text\": \"{title[i]}\" }}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4658df8-96d0-4b58-b89a-9c8285bc44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_jsonl(os.getenv('DATA2') + '/topics/topic-j0101-0149.utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684bec1e-911e-4efb-a27d-0a3b3e9dd55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic-e0101-0149  topic-j0101-0149.utf8\n",
      "topic-j0101-0149  topic-j0101-0149.utf8.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA2/topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87e486-71c2-423e-afaa-3fa2795d2a82",
   "metadata": {},
   "source": [
    "## Top 1000 data\n",
    "\n",
    "- NTCIR-17 Transfer Task Participant only (for Reranking subtask)\n",
    "- Download `top1000.eval.tsv` into `../testcollections/ntcir/NTCIR-2/j-docs` folder\n",
    "- Note that not all topics have 1000 docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13935ac0-61b0-4e35-b8b1-345d15c1d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntc1-j1.utf8.mod.jsonl\tntc2-j1g.utf8\t     ntc2-j1k.utf8\n",
      "ntc12-j1gk.mod.jsonl\tntc2-j1g.utf8.jsonl  ntc2-j1k.utf8.jsonl\n",
      "ntc2-j1g\t\tntc2-j1k\t     top1000.eval.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA2/j-docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c36b89-e7de-4b34-9559-ce4cbd42bdbf",
   "metadata": {},
   "source": [
    "## Register to ir_datasets module locally\n",
    "\n",
    "- Dataset name: `ntcir-transfer`\n",
    "- subset: `1/eval`\n",
    "- No qrels\n",
    "\n",
    "### Location of dataset files\n",
    "\n",
    "- `../datasets/ntcir-transfer.yaml`\n",
    "- `../datasets/ntcir_transfer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8a53c0f-ccfd-47eb-ba77-7875efff22b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old cache (if any)\n",
    "# !rm -rf ~/.ir_datasets/ntcir-transfer/1/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34fca823-3bce-40dd-a279-41670939e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q ir_datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1180ce-69bf-4eea-8261-9e90931d611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.abspath('__file__')), '../datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4f20b17-b186-44d5-a7f3-ad56193f7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import ntcir_transfer\n",
    "dataset = ir_datasets.load('ntcir-transfer/1/eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93227a65-5b2b-4b97-9c02-b606543013c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('doc_id', str), ('text', str)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.docs_cls().__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefa7f1-eefa-48f5-bc92-85d18161b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = dataset.docs_store()\n",
    "docstore.get('kaken-j-0924516300').text # the one in the overview paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bab922f5-3fca-4d67-bfdf-eec96b07d0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('query_id', str), ('text', str)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.queries_cls().__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2484b3a9-f42c-4093-b192-91a75777a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101</td>\n",
       "      <td>kaken-j-0975101400</td>\n",
       "      <td>17.722502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101</td>\n",
       "      <td>kaken-j-0960142800</td>\n",
       "      <td>17.664987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0101</td>\n",
       "      <td>kaken-j-0911436000</td>\n",
       "      <td>17.568185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101</td>\n",
       "      <td>kaken-j-0970425300</td>\n",
       "      <td>17.503291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101</td>\n",
       "      <td>kaken-j-0934033100</td>\n",
       "      <td>17.445929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43635</th>\n",
       "      <td>0149</td>\n",
       "      <td>kaken-j-0972466500</td>\n",
       "      <td>-17.015109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43636</th>\n",
       "      <td>0149</td>\n",
       "      <td>kaken-j-0960134100</td>\n",
       "      <td>-17.017582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43637</th>\n",
       "      <td>0149</td>\n",
       "      <td>gakkai-j-0000185751</td>\n",
       "      <td>-17.018490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43638</th>\n",
       "      <td>0149</td>\n",
       "      <td>kaken-j-0904518400</td>\n",
       "      <td>-17.031096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43639</th>\n",
       "      <td>0149</td>\n",
       "      <td>kaken-j-0972126200</td>\n",
       "      <td>-17.031747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43640 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id               doc_id      score\n",
       "0         0101   kaken-j-0975101400  17.722502\n",
       "1         0101   kaken-j-0960142800  17.664987\n",
       "2         0101   kaken-j-0911436000  17.568185\n",
       "3         0101   kaken-j-0970425300  17.503291\n",
       "4         0101   kaken-j-0934033100  17.445929\n",
       "...        ...                  ...        ...\n",
       "43635     0149   kaken-j-0972466500 -17.015109\n",
       "43636     0149   kaken-j-0960134100 -17.017582\n",
       "43637     0149  gakkai-j-0000185751 -17.018490\n",
       "43638     0149   kaken-j-0904518400 -17.031096\n",
       "43639     0149   kaken-j-0972126200 -17.031747\n",
       "\n",
       "[43640 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(dataset.scoreddocs_iter())"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
